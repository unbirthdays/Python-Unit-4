{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "vital-resident",
   "metadata": {},
   "source": [
    "# Multivariate Time Series Analysis Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "offshore-reach",
   "metadata": {},
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "searching-avenue",
   "metadata": {},
   "source": [
    "This lab will help you properly analyze a **stationary** time series dataset, which means in part that the data:\n",
    "\n",
    "- Is **not** seasonal\n",
    " - lacks regular and predictable changes recurring every year\n",
    "- Has **no** trend\n",
    " - lacks long term upward or downward movement\n",
    "\n",
    "> **Note:** Time series with seasonality and trends require additional analysis even though the (S)ARIMA(X) method used here can handle them.\n",
    "\n",
    "### What You'll DO\n",
    "\n",
    "<span style='background :yellow' > Important, key lines of code will be missing portions marked by **T_O_D_O** for you to fill in.</span>\n",
    "\n",
    "Review the lectures and package documentation (referenced throughout the lab) to apply what you learn and fill in these gaps. \n",
    "\n",
    "The primary lectures this lab builds upon:\n",
    "\n",
    "- *Mining Data from Time Series*\n",
    "- *Understanding and Applying Linear Regression*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "possible-retreat",
   "metadata": {},
   "source": [
    "### About the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "formed-toyota",
   "metadata": {},
   "source": [
    "The data spans from 1970 Q1 to 2019 Q2 and is an economic times series of quarterly percentage changes (growth rates) of five variables:\n",
    "\n",
    "- personal **consumption** expenditure\n",
    "- personal disposable **income**\n",
    "- personal **savings**\n",
    "- industrial **production**\n",
    "- **unemployment**\n",
    "\n",
    "Consumption, income, savings, and production are in real terms (i.e. inflation adjusted).\n",
    "\n",
    "**Source:** United States Federal Reserve Bank of St Louis and the `fpp3` package in R. You can read more about this dataset and the analysis of it in R from the free and open source textbook: [Forecasting: Principles and Practice (3rd ed)](https://otexts.com/fpp3/regression-intro.html#example-us-consumption-expenditure) by Rob J Hyndman and George Athanasopoulos. This lab gives you the starter code and guidance to analyze the dataset in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equal-portugal",
   "metadata": {},
   "source": [
    "### Python Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "behind-position",
   "metadata": {},
   "source": [
    "Ensure the following packages are installed into a Python 3.8.5 environment:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "marine-senior",
   "metadata": {},
   "source": [
    "| Package| Purpose |\n",
    "| :-- | :-- |\n",
    "| `pandas` | data wrangling and plotting |\n",
    "| `seaborn` | plotting |\n",
    "| `matplotlib` | tweaking seaborn and pandas plots |\n",
    "| `pmdarima` | aide for model parameter selection |\n",
    "| `statsmodels` | time series analysis tools |\n",
    "| `numpy` | data manipulation |\n",
    "| `scipy` | to visualize a normal distribution |\n",
    "| `jupyter` | running this notebook |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "catholic-contamination",
   "metadata": {},
   "source": [
    "#### Installation Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incomplete-sunset",
   "metadata": {},
   "source": [
    "**Step 1**: Using `conda` create that environment using:\n",
    "    \n",
    "```\n",
    "conda create -n tsa python=3.8.5 pandas seaborn matplotlib statsmodels numpy scipy jupyter\n",
    "```\n",
    "\n",
    "**Step 2**: Install `pmdarima` separately after activating the environment using `pip`:\n",
    "\n",
    "```\n",
    "conda activate tsa\n",
    "pip install pmdarima\n",
    "```\n",
    "\n",
    "**Step 3**: Launch this notebook from within the directory that contains it:\n",
    "\n",
    "```\n",
    "jupyter notebook\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flush-burlington",
   "metadata": {},
   "source": [
    "## Data Wrangling\n",
    "\n",
    "Load the datasets and configure the pandas dataframe to use a time [PeriodIndex](https://pandas.pydata.org/docs/reference/api/pandas.PeriodIndex.html). Use the [pd.period_range](https://pandas.pydata.org/docs/reference/api/pandas.period_range.html?highlight=period_range#pandas.period_range) function to do this and make sure it matches the values of the `Quarter` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outstanding-production",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"us_consumption.csv\", header = 0, index_col = 0)\n",
    "df.index = pd.period_range(T_O_D_O)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "combined-sydney",
   "metadata": {},
   "source": [
    "Next, drop the Quarter column from the dataframe as we won't need it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trying-dynamics",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(T_O_D_O)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "creative-wallet",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)\n",
    "\n",
    "Look at the time series data visualized as a line plot over time using `pandas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "systematic-validation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "df.plot.line(subplots=True, figsize=(12,12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "changed-boating",
   "metadata": {},
   "source": [
    "Visualize the data without the time component by seeing the [paired scatter plots](https://seaborn.pydata.org/generated/seaborn.pairplot.html) of the relationships between these five variables with `seaborn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automated-sacramento",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "T_O_D_O\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "falling-privacy",
   "metadata": {},
   "source": [
    "### Think about the data\n",
    "\n",
    "What relationships do you see between the variables in both plots above? Do you think a regression analysis will be useful on this data set? These questions can help inform on what kind of model would work best on this. This lab will use the AIRMA model introduced in the lectures. Other models could also work on this dataset, but those are outside the scope of this lab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "technical-language",
   "metadata": {},
   "source": [
    "## Building an AIRMA model with exogenous variables\n",
    "\n",
    "> **Note:** This kind of model is also known as a dynamic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "casual-blond",
   "metadata": {},
   "source": [
    "### Preprocessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hungry-devices",
   "metadata": {},
   "source": [
    "Use `.iloc` to [slice out](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.iloc.html) the four predictor variables (income, production, savings, employment). You should see printed out a dataframe with the period index followed by columns of the these variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phantom-maryland",
   "metadata": {},
   "outputs": [],
   "source": [
    "perdictors = T_O_D_O\n",
    "predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bronze-gamma",
   "metadata": {},
   "source": [
    "Use .iloc to splice out the response variable, *Consumption*. You should see printed out a dataframe with the period index followed by the response variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trained-python",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = T_O_D_O\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "every-pennsylvania",
   "metadata": {},
   "source": [
    "### Using an AutoML tool to find AIRMA parameters "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "german-enzyme",
   "metadata": {},
   "source": [
    "Here we'll use the `pmdarima` package to help us find the `p`, `d`, and `q` variables of the ARIMA model. \n",
    "\n",
    "We need `suppress_warngins=False` we want to ensure that there are no failed to converge warnings as the model won't be reliable if convergence isn't achieved. In order to do that, we'll need to set `maxitter` higher than the default of 50. Beware that higher values will take longer to run, so some patience will be needed.\n",
    "\n",
    "When `maxiter` is set high enough you should not see any convergence warnings. \n",
    "\n",
    "> **Note:** Depending on the parameters used with `auto_arima` you might see other warnings like a deprecation and/or a non-invertible starting MA parameters found warning. You can ignore these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "level-specialist",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pmdarima as pm\n",
    "auto = pm.auto_arima(y=response, exogenous=predictors, suppress_warnings=False, \n",
    "                     stationary=True, maxiter=T_O_D_O)\n",
    "auto.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "immune-designation",
   "metadata": {},
   "source": [
    "### Setting ARIMA parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pregnant-diamond",
   "metadata": {},
   "source": [
    "Use the model above to set the `sm.tsa.SARIMAX(p, d, q)` paremeters below and fit the model to the data using the same `maxiter` value used above. \n",
    "\n",
    "The [SARIMAX documentation](https://www.statsmodels.org/devel/generated/statsmodels.tsa.statespace.sarimax.SARIMAX.html) may prove helpful.\n",
    "\n",
    "> **Note:** The output of this cell should match the previous (or be very close). If it doesn't check to see if there is an intercept term missing from the model above, but showing in the output below. If that's the case, remove `trend='c'` which specifies to add an intercept term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "harmful-chess",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "model = sm.tsa.SARIMAX(endog=response, exog=predictors, trend='c', order=T_O_D_O)\n",
    "result = model.fit(T_O_D_O, disp=0)\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chemical-penny",
   "metadata": {},
   "source": [
    "## Model Diagnostics\n",
    "\n",
    "Always check the assumptions of a statistical model. Failure to do so can lead to unreliable inferences, predictions, and forecasts. The more assumptions that are violated, the less usable the models are. In real world datasets, it's common for one or more assumptions to be violated. This isn't neccessarily a problem if the violation is minor. If it's major you'll likely need to look into statistical methods to remedy violations which we won't cover here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "independent-melissa",
   "metadata": {},
   "source": [
    "### Assumption 1: No Residual Autocorrelations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "structural-tanzania",
   "metadata": {},
   "source": [
    "Check the autocorrelations of the model residuals to ensure there all non-zerio lags don't fall grossly outside of the white noise zone (which would be the blue shading of the plot). You should see lag 7 slightly outside the range, but this shouldn't pose a big issue.\n",
    "\n",
    "To do this you'll need to use the plot_acf function from `statsmodels`. Fill in the missing parts below to display 20 lags along the x-axis. You'll need to provide two parameters to the `plot_acf` method to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "removable-airfare",
   "metadata": {},
   "outputs": [],
   "source": [
    "def autocorrelation_plot(data):\n",
    "    from T_O_D_O import plot_acf\n",
    "    fig = plt.figure(figsize=(12, 4))\n",
    "    ax1 = fig.gca()    \n",
    "    plot_acf(T_O_D_O, ax=ax1)\n",
    "    plt.show()\n",
    "    \n",
    "autocorrelation_plot(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "taken-progress",
   "metadata": {},
   "source": [
    "### Assumption 2: No Excessive Residual Skew or Kurtosis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "running-korean",
   "metadata": {},
   "source": [
    "This function is directly from the *Mining Data from Time Series* lecture. What we want to see from it's output is a histogram that resembles:\n",
    "\n",
    "- a normal distribution wouthout too much kurtosis (not too tall and skinny in the middle) \n",
    "- and isn't skewed to the left or right too much (peak should be centered with the superimposed normal distribution's peak)\n",
    "\n",
    "If the historgram deviates too much from these ideals it may mean our model will not be reliable for forecasting.\n",
    "\n",
    "How do you feel about the distribution visualized belwo for this model below?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wanted-matter",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resid_histogram(data):\n",
    "    from numpy import linspace\n",
    "    from scipy.stats import norm\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.hist(data, bins = 'auto', density = True, rwidth = 0.85, label ='Residuals')\n",
    "    mean_resid, std_resid = norm.fit(data)\n",
    "    xmin, xmax = plt.xlim()\n",
    "    curve_length = linspace(xmin, xmax, 100)\n",
    "    bell_curve = norm.pdf(curve_length, mean_resid, std_resid)\n",
    "    plt.plot(curve_length, bell_curve, 'm', linewidth = 2)\n",
    "    plt.grid(axis='y', alpha = 0.2)\n",
    "    plt.xlabel('Residual')\n",
    "    plt.ylabel('Density')\n",
    "    plt.title(f'Residuals vs Normal Distribution - Mean {round(mean_resid, 1)}, Std = {round(std_resid, 1)}')\n",
    "    plt.show()\n",
    "    \n",
    "resid_histogram(result.resid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organic-kenya",
   "metadata": {},
   "source": [
    "### Assumption 3: Residual Constant Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "based-beginning",
   "metadata": {},
   "source": [
    "Regression models assume the model residuals have constant variance ([homoskedasticity](https://www.investopedia.com/terms/h/homoskedastic.asp#:~:text=What%20is%20Homoskedastic%3F,of%20the%20predictor%20variable%20changes)). Use pandas to perform a line plot of the residuals overtime by filling in the missing code below\n",
    "\n",
    "Do you think these residuals have constant variance or are they subject to large changes?\n",
    "\n",
    "> **Note:** These plots typically use standardized residuals to exaggerate the differences to make this more clear and we'll see an example of such a diagnostic plot later, but the purpose here is to help you understand what you are visualizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "textile-philippines",
   "metadata": {},
   "outputs": [],
   "source": [
    "def residuals_over_time(data):\n",
    "    fig = plt.figure(figsize=(12, 6))\n",
    "    ax1 = fig.gca()\n",
    "    T_O_D_O\n",
    "    plt.title('Residuals Over Time')\n",
    "    plt.show()\n",
    "    \n",
    "residuals_over_time(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cooked-distance",
   "metadata": {},
   "source": [
    "### Assumption 4: Residuals vs Fitted Values Randomly Scattered\n",
    "\n",
    "What we want to see here is no clear trend or pattern, no correlation between the x and y axis.\n",
    "\n",
    "Create a scatterplot of the fitted values for the x-axis and the residual values for the y-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mysterious-account",
   "metadata": {},
   "outputs": [],
   "source": [
    "def residuals(data):\n",
    "    fig = plt.figure(figsize=(12, 6))\n",
    "    ax1 = fig.gca()\n",
    "    sns.scatterplot(T_O_D_O, ax=ax1)\n",
    "    plt.title('Residudals vs Fitted Values')\n",
    "    plt.ylabel('Residual')\n",
    "    plt.xlabel('Fitted Value')\n",
    "    plt.show()\n",
    "    \n",
    "residuals(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "another-oxygen",
   "metadata": {},
   "source": [
    "## Diagnostics Convience Function\n",
    "\n",
    "`Statsmodels` comes with one-liner `plot_diagnositics` funtion that gives us 3 of the 4 above plots with some modifications:\n",
    "\n",
    "- Residuals over time are standardized making the variation easy to notice\n",
    "- Historgram includes a kernal denesity estimation (`KDE`) curve to help visualize how closly the histogram resembles the normal distribution (`N(0,1)`)\n",
    "- Correlogram is another name for ACF\n",
    "\n",
    "Additionally, it also shows a Q-Q plot, which is another common way to check if the residuals or normally distributed. The dots should line up mostly along the diagonal line. The further they deviate from that line the more the normality assumption is violated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "original-afternoon",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16, 9))\n",
    "result.plot_diagnostics(fig=fig, lags=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "logical-purple",
   "metadata": {},
   "source": [
    "## Scenerio Forecasting\n",
    "\n",
    "Lastly, we'll use this model to forcast changes in US consumption over the next year, with the scenerio of unemployment growth rates increase linearly from 0.1% to 1%. You'll see below all other predictor variables are held constant with only the unemployment variable changing. This gives us some idea of the impact unemployement rates have on personal consumption rates in the US.\n",
    "\n",
    "> **NOTE:** When forecasting scenerios like this, beware of the risk of extrapolating the forecasts beyond what the model can handle. In this case, check the first EDA plot above and you'll see the unemployment percent changes have stayed mostly between -1% and +1%. Thus ensure forecasts use scenerios that use values between these ranges, not outside of them. Doing the latter will lead to unreliable forecasts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "capable-sperm",
   "metadata": {},
   "outputs": [],
   "source": [
    "current=predictors.iloc[-1,:]\n",
    "scenerio=[]\n",
    "num_quarters = 4\n",
    "for i in range(num_quarters):\n",
    "    current = current.copy()\n",
    "    new_unemployment_rate = 1.0\n",
    "    current.Unemployment += (new_unemployment_rate - current.Unemployment) / (num_quarters-1) * i\n",
    "    scenerio.append(current)\n",
    "scenerio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "artificial-stock",
   "metadata": {},
   "source": [
    "To visualize the effect of the unemployment rate changing use the [`get_forecast`](https://www.statsmodels.org/devel/examples/notebooks/generated/statespace_forecasting.html) method of the `statsmodels` object. Fill in the missing code to forecast 4 quarters into the future using the scenerio defined above. \n",
    "\n",
    "The forecast below shows:\n",
    "\n",
    "- the historical time series of consumption in blue,\n",
    "- a point forecast (black line showing the most likely result), and \n",
    "- a 95% prediction interval (in grey shading showing that we are 95% confident that other possible outcomes will not exceed the shaded area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ranking-berlin",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "response.plot(ax=ax)\n",
    "fcast = result.get_forecast(T_O_D_O).summary_frame()\n",
    "fcast['mean'].plot(ax=ax, style='k-')\n",
    "ax.fill_between(fcast.index, fcast['mean_ci_lower'], fcast['mean_ci_upper'], color='k', alpha=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transsexual-logic",
   "metadata": {},
   "source": [
    "Given what you can observe about the data in the EDA section and what you know about the economy in general, does this forcast seem reasonable? \n",
    "\n",
    "## Parting Ideas\n",
    "\n",
    "Feel free to play around with some other scenerios and see how they affect the forecast. \n",
    "\n",
    "Or try to tweak the ARIMA parameters (e.g. `p, q, d = 4, 5, 4`) to see how it affects the diagnostic plots and forecasts.\n",
    "\n",
    "This ends the lab."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
